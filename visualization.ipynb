{"cells":[{"cell_type":"markdown","metadata":{"id":"zroCkbVdTi-1"},"source":["# Plot Figures"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35010,"status":"ok","timestamp":1727826442690,"user":{"displayName":"David Xu","userId":"15485235073676026350"},"user_tz":240},"id":"GvUk0gvRK5qX","outputId":"2fe389ad-6cae-4537-f510-833d9fee73f3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install adjustText"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1727826442691,"user":{"displayName":"David Xu","userId":"15485235073676026350"},"user_tz":240},"id":"9pAKLzJkJ8Zs"},"outputs":[],"source":["model_paramsize = {\n","        \"openai/gpt-4o-mini\": 8 * 10**9,\n","        \"openai/gpt-4o\": 200 * 10**9,\n","        \"openai/gpt-4-turbo\": 1760 * 10**9,\n","        \"openai/gpt-3.5-turbo-0125\": 20 * 10**9,\n","        \"anthropic/claude-2\": 137 * 10**9,\n","        \"anthropic/claude-3-haiku\": 20 * 10**9,\n","        \"anthropic/claude-3-sonnet\": 70 * 10**9,\n","        \"anthropic/claude-3-opus\": 2000 * 10**9,\n","        \"google/gemini-pro\" : 20 * 10**9,\n","        \"google/gemini-pro-1.5\" : 175 * 10**9,\n","        \"meta-llama/llama-3-8b-instruct\": 8 * 10**9,\n","        \"meta-llama/llama-3-70b-instruct\": 70 * 10**9,\n","        \"mistralai/mistral-7b-instruct:nitro\": 7 * 10**9,\n","        \"mistralai/mixtral-8x7b-instruct\": 56 * 10**9,\n","        \"mistralai/mixtral-8x22b-instruct\": 176 * 10**9,\n","        \"openai/o1-mini\": 120 * 10**9,\n","        \"openai/o1-preview\": 480 * 10**9,\n","        \"meta-llama/llama-3.1-8b-instruct\": 8 * 10**9,\n","        \"meta-llama/llama-3.1-70b-instruct\": 70 * 10**9,\n","        \"meta-llama/llama-3.1-405b-instruct\": 405 * 10**9,\n","        \"mistralai/mistral-large\": 123 * 10**9,\n","        \"mistralai/mistral-small\": 22 * 10**9,\n","        \"mistralai/mistral-tiny\": 7 * 10**9,\n","        \"human_review2\": 0\n","    }\n","\n","model_type = {\n","        \"openai/gpt-4o-mini\": \"D gpt\",\n","        \"openai/gpt-4o\": \"D gpt\",\n","        \"openai/gpt-4-turbo\": \"D gpt\",\n","        \"openai/gpt-3.5-turbo-0125\": \"D gpt\",\n","        \"anthropic/claude-2\": \"B claude2\",\n","        \"anthropic/claude-3-haiku\": \"A claude3\",\n","        \"anthropic/claude-3-sonnet\": \"A claude3\",\n","        \"anthropic/claude-3-opus\": \"A claude3\",\n","        \"google/gemini-pro\" : \"C gemini\",\n","        \"google/gemini-pro-1.5\" : \"C gemini\",\n","        \"meta-llama/llama-3-8b-instruct\": \"F llama3\",\n","        \"meta-llama/llama-3-70b-instruct\": \"F llama3\",\n","        \"mistralai/mistral-7b-instruct:nitro\": \"mistral\",\n","        \"mistralai/mixtral-8x7b-instruct\": \"H mixtral\",\n","        \"mistralai/mixtral-8x22b-instruct\": \"H mixtral\",\n","        \"openai/o1-preview\": \"o1\",\n","        \"openai/o1-mini\": \"o1\",\n","        \"meta-llama/llama-3.1-8b-instruct\": \"E llama3.1\",\n","        \"meta-llama/llama-3.1-70b-instruct\": \"E llama3.1\",\n","        \"meta-llama/llama-3.1-405b-instruct\": \"E llama3.1\",\n","        \"mistralai/mistral-large\": \"G mistral\",\n","        \"mistralai/mistral-small\": \"G mistral\",\n","        \"mistralai/mistral-tiny\": \"G mistral\",\n","        \"human_review2\": \"human\",\n","    }\n","\n","model_nickname = {\n","        \"openai/gpt-4o-mini\": \"GPT-4o-mini\",\n","        \"openai/gpt-4o\": \"GPT-4o\",\n","        \"openai/gpt-4-turbo\": \"GPT-4-turbo\",\n","        \"openai/gpt-3.5-turbo-0125\": \"GPT-3.5-turbo\",\n","        \"anthropic/claude-2\": \"Claude-2\",\n","        \"anthropic/claude-3-haiku\": \"Claude-3-haiku\",\n","        \"anthropic/claude-3-sonnet\": \"Claude-3-sonnet\",\n","        \"anthropic/claude-3-opus\": \"Claude-3-opus\",\n","        \"google/gemini-pro\" : \"Gemini-pro-1\",\n","        \"google/gemini-pro-1.5\" : \"Gemini-pro-1.5\",\n","        \"meta-llama/llama-3-8b-instruct\": \"Llama-3-8b\",\n","        \"meta-llama/llama-3-70b-instruct\": \"Llama-3-70b\",\n","        \"mistralai/mistral-7b-instruct:nitro\": \"Mistral-7b\",\n","        \"mistralai/mixtral-8x7b-instruct\": \"Mixtral-8x7b\",\n","        \"mistralai/mixtral-8x22b-instruct\": \"Mixtral-8x22b\",\n","        \"openai/o1-preview\": \"O1-preview\",\n","        \"openai/o1-mini\": \"O1-mini\",\n","        \"meta-llama/llama-3.1-8b-instruct\": \"Llama-3.1-8b\",\n","        \"meta-llama/llama-3.1-70b-instruct\": \"Llama-3.1-70b\",\n","        \"meta-llama/llama-3.1-405b-instruct\": \"Llama-3.1-405b\",\n","        \"mistralai/mistral-large\": \"Mistral-large\",\n","        \"mistralai/mistral-small\": \"Mistral-small\",\n","        \"mistralai/mistral-tiny\": \"Mistral-tiny\",\n","        \"human_review2\": \"human\"\n","    }"]},{"cell_type":"markdown","metadata":{"id":"xcN69nCudCYH"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1605,"status":"ok","timestamp":1727826449041,"user":{"displayName":"David Xu","userId":"15485235073676026350"},"user_tz":240},"id":"HSFqDl6AdD7F"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from adjustText import adjust_text\n","import json\n","import matplotlib.lines as mlines\n","\n","task = \"iclr23\"\n","working_dir = f\"/content/drive/MyDrive/llmserver/\"\n","task_dir = f\"/content/drive/MyDrive/llmserver/task_{task}/\"\n","llmgen_cache_dir = working_dir + \"cache_llmgen/\"\n","logprobs_cache_dir = working_dir + \"cache_logprobs/\"\n","finetune_cache_dir = working_dir + \"cache_finetune/\"\n","dataset_dir = task_dir + \"dataset/\"\n","result_dir = task_dir + \"result/\"\n","\n","csv_dir = working_dir\n","\n","ft = False\n","\n","with open(f\"{result_dir}abstract_result{'_ft' if ft else ''}.json\", 'r', encoding='utf-8') as json_file:\n","    res_dict = json.load(json_file)"]},{"cell_type":"markdown","metadata":{"id":"pN7wmGrHc6S6"},"source":["# Save to CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":705,"status":"ok","timestamp":1727779242350,"user":{"displayName":"Yuxuan Lu","userId":"16491502565075368087"},"user_tz":-480},"id":"HRK_R1gwc4CI","outputId":"4857437b-ea8e-40e1-c6e9-c36d77079abd"},"outputs":[],"source":["df = pd.DataFrame(list(model_paramsize.items()), columns=['Model Name', 'Parameter Size'])\n","\n","GEM_score = []\n","GEMS_score = []\n","\n","judge = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","GEM_score_dict = res_dict[f\"GEM-{judge}\"]\n","GEMS_score_dict = res_dict[f\"GEMS-{judge}\"]\n","\n","print(GEM_score_dict)\n","print(GEMS_score_dict)\n","\n","for model in model_paramsize.keys():\n","    if model not in GEM_score_dict.keys() or model not in GEMS_score_dict.keys():\n","        GEM_score.append(0)\n","        GEMS_score.append(0)\n","        continue\n","    GEM_score.append(GEM_score_dict[model][0])\n","    GEMS_score.append(GEMS_score_dict[model][0])\n","\n","df['GEM Score'] = GEM_score\n","df['GEMS Score'] = GEMS_score\n","\n","df.to_excel(csv_dir + \"GREbench.xlsx\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"pKONe2CS2xlk"},"source":["# Correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9448,"status":"ok","timestamp":1727826460329,"user":{"displayName":"David Xu","userId":"15485235073676026350"},"user_tz":240},"id":"h-nTFn-72zKV","outputId":"2c61b5c1-e3fe-4db1-e597-4c6012d507f5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy.stats import pearsonr\n","\n","file_path = working_dir + \"GRE-bench.xlsx\"\n","df = pd.read_excel(file_path)\n","\n","original_columns = df.columns.tolist()\n","original_columns.remove('model_name')\n","original_columns.remove('parameter_size')\n","# score_columns = df.columns.difference(['model_name','parameter_size'])\n","\n","correlation_matrix = pd.DataFrame(index=original_columns, columns=original_columns)\n","\n","for col1 in original_columns:\n","    for col2 in original_columns:\n","        if col1 == col2:\n","            correlation_matrix.loc[col1, col2] = 1.0\n","        else:\n","            non_na = df[[col1, col2]].dropna()\n","            if non_na.shape[0] > 1:\n","                corr, _ = pearsonr(non_na[col1], non_na[col2])\n","                correlation_matrix.loc[col1, col2] = corr\n","            else:\n","                correlation_matrix.loc[col1, col2] = np.nan\n","\n","correlation_matrix = correlation_matrix.astype(float)\n","\n","plt.rc('font', size=20) \n","plt.rc('axes', titlesize=30)  \n","plt.rc('axes', labelsize=30)  \n","plt.rc('xtick', labelsize=30) \n","plt.rc('ytick', labelsize=30)  \n","plt.rc('legend', fontsize=30)  \n","plt.rc('figure', titlesize=30)\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, annot=True, cmap='magma', center=0)\n","plt.title('Correlation Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XrDuBdcDc06K"},"source":["# Main Figure"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":632},"executionInfo":{"elapsed":5019,"status":"ok","timestamp":1727827507701,"user":{"displayName":"David Xu","userId":"15485235073676026350"},"user_tz":240},"id":"CaULwtkBGU5-","outputId":"88970164-1978-43f4-f0be-ee7ff60462cc"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import matplotlib.colors as mcolors\n","\n","label_fontsize = 30\n","text_fontsize = 24\n","tick_fontsize = 24\n","marker_size = 100 \n","line_width = 2\n","\n","fig, ax = plt.subplots(ncols=4, figsize=(28, 12), gridspec_kw={'width_ratios': [10, 1.3, 10, 0.7]})\n","\n","for method in [\"GEM\", \"GEMS\"]:\n","    if ft:\n","        judge = \"/content/drive/MyDrive/llmserver/cache_finetune/Meta-Llama-3.1-8B-bnb-4bit-finetune-peer-review-judge\"\n","    else:\n","        judge = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","    which_score = f\"{method}-{judge}\"\n","    score_dict = res_dict[which_score]\n","\n","    human_score = score_dict['human_review2'][0]\n","    human_std = score_dict['human_review2'][1]\n","    human_lower = human_score - 1.645 * human_std\n","    human_upper = human_score + 1.645 * human_std\n","\n","    model_scores = {k: v for k, v in score_dict.items() if k != 'human_review2'}\n","\n","    sorted_models = sorted(model_scores.keys(), key=lambda x: (model_type[x], - model_paramsize[x]), reverse=True)\n","\n","    scores = [model_scores[model][0] for model in sorted_models]\n","    errors = [1.645 * model_scores[model][1] for model in sorted_models] \n","    types = [model_type[model] for model in sorted_models]\n","    paramsizes = [model_paramsize[model] for model in sorted_models]\n","\n","    bar_height = 4\n","    type_gap = 6\n","\n","    positions = []\n","    current_position = 0\n","\n","    for i in range(len(sorted_models)):\n","        if i == 0:\n","            positions.append(current_position)\n","        else:\n","            if types[i] == types[i - 1]:\n","                current_position += bar_height\n","            else:\n","                current_position += type_gap\n","            positions.append(current_position)\n","\n","    print(positions)\n","\n","    norm = mcolors.LogNorm(vmin=min(paramsizes), vmax=max(paramsizes))\n","    cmap = cm.viridis\n","    mappable = cm.ScalarMappable(norm=norm, cmap=cmap)\n","    colors = [cmap(norm(size)) for size in paramsizes]\n","\n","    axis = ax[0 if method == \"GEM\" else 2]\n","\n","    if method == \"GEM\":\n","        bars = axis.barh(positions, [-score for score in scores], height=bar_height, xerr=errors, capsize=5, color=colors)\n","    else:\n","        bars = axis.barh(positions, scores, height=bar_height, xerr=errors, capsize=5, color=colors)\n","\n","    if method == \"GEM\":\n","        axis.axvline(- human_score, color='gray', linestyle='-', linewidth=2)\n","        axis.axvspan(- human_upper, - human_lower, facecolor='lightgrey', alpha=0.5)\n","    else:\n","        axis.axvline(human_score, color='gray', linestyle='-', linewidth=2)\n","        axis.axvspan(human_lower, human_upper, facecolor='lightgrey', alpha=0.5)\n","\n","    if method == \"GEMS\":\n","        axis.set_ylim(positions[0] - bar_height / 2 - 2,positions[-1] + bar_height / 2 + 2)\n","        axis.set_yticks(positions)\n","        axis.set_yticklabels([\"\" for s in sorted_models], fontsize=tick_fontsize)\n","\n","        ax[1].set_axis_off()\n","        ax[1].set_ylim(positions[0] - bar_height / 2 - 2,positions[-1] + bar_height / 2 + 2)\n","        ax[1].set_xlim(-15,15)\n","        for pos, label in zip(positions, [model_nickname[s] for s in sorted_models]):\n","            ax[1].text(x=0, y=pos, s=\" \" + label + \" \", va='center', ha='center', fontsize=tick_fontsize) \n","    else:\n","        ax_right = axis.twinx()\n","        axis.set_ylim(positions[0] - bar_height / 2 - 2,positions[-1] + bar_height / 2 + 2)\n","        ax_right.set_ylim(positions[0] - bar_height / 2 - 2,positions[-1] + bar_height / 2 + 2)\n","        axis.set_yticks([])\n","        ax_right.set_yticks(positions)\n","        ax_right.set_yticklabels([\"\" for s in sorted_models], fontsize=tick_fontsize)\n","        \n","\n","    if method == \"GEM\" and not ft:\n","        axis.set_xlim(-65, -35)\n","        axis.set_xticks([-65, -60, -55, -50, -45, -40, -35])\n","        axis.set_xticklabels([\"65\", \"60\", \"55\", \"50\", \"45\", \"40\", \"35\"])\n","    elif method == \"GEMS\" and not ft:\n","        axis.set_xlim(10, 30)\n","        axis.set_xticks([10, 15, 20, 25, 30])\n","    elif method == \"GEM\" and ft:\n","        axis.set_xlim(-32, -12)\n","        axis.set_xticks([-32, -28, -24, -20, -16, -12])\n","        axis.set_xticklabels([\"32\", \"28\", \"24\", \"20\", \"16\",\"12\"])\n","    elif method == \"GEMS\" and ft:\n","        axis.set_xlim(0, 14)\n","        axis.set_xticks([0,2,4,6,8,10,12,14])\n","\n","    axis.set_xlabel('GRE Bench (GEM-finetune)' if method == \"GEM\" else \"GRE Bench (GEM-S-finetune)\", fontsize=label_fontsize)\n","    axis.tick_params(axis='x', labelsize=tick_fontsize)\n","    axis.tick_params(axis='y', labelsize=tick_fontsize)\n","\n","cbar = plt.colorbar(mappable, cax=ax[3], orientation='vertical')\n","cbar.set_label('Parameter Size', fontsize=label_fontsize)\n","cbar.ax.tick_params(labelsize=tick_fontsize)\n","\n","plt.savefig('/content/drive/MyDrive/llmserver/GRE-bench.png')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}}},"nbformat":4,"nbformat_minor":0}
